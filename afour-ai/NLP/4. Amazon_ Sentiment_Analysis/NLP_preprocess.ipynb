{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abulf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abulf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abulf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\abulf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting : 100%|██████████| 92/92 [00:03<00:00, 28.92it/s] \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "sys.path.append(str(Path(os.getcwd()).parents[1]))\n",
    "from Helper_Functions import common_utils\n",
    "directory_to_extract_to = os.getcwd()\n",
    "\n",
    "common_utils.load_data_from_one_drive(directory_to_extract_to, \"nlp_paths\", \"Amazon_sentimental_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence: str) -> list:\n",
    "  # Remove the review tag of HTML\n",
    "  tags = re.compile(\"(<review_text>|<\\/review_text>)\")\n",
    "  sentence = re.sub(tags, '', sentence)\n",
    "  sentence = sentence.lower()\n",
    "  # Remove emails and urls\n",
    "  email_urls = re.compile(\"(\\bhttp.+? | \\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b)\")\n",
    "  sentence = re.sub(email_urls, '', sentence)\n",
    "  # Some used '@' to hide offensive words (bla -> bl@)\n",
    "  ats = re.compile('@')\n",
    "  sentence = re.sub(ats, 'a', sentence)\n",
    "  # Remove Punctuation \n",
    "  #punc = re.compile(\"[^\\w\\s(\\w+\\-\\w+)]\")\n",
    "  #punc = re.compile(\"[!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+,\\-\\.\\/\\:;<=>\\?\\[\\\\\\]\\^_`\\{\\|\\}\\~]\")\n",
    "  sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "  # Remove digits\n",
    "  pattern = r'[0-9]'\n",
    "  sentence = re.sub(pattern, '', sentence)\n",
    "  # Remove stopwords and tokenize\n",
    "  # sentence = sentence.split(sep=' ')\n",
    "  sentence = word_tokenize(sentence)\n",
    "  sentence = [word for word in sentence if not word in stopwords.words()]\n",
    "  #lemmatizer = WordNetLemmatizer()\n",
    "  #sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "  return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/sorted_data_acl/\"#Here I used the Relative path if this cell shows error then try to use the absolute path I gurantee it does not show any error  \n",
    "regex_review = re.compile(\"<review_text>.+?<\\/review_text>\", flags=re.DOTALL)\n",
    "\n",
    "Collections_test = defaultdict(list)\n",
    "# Training Data\n",
    "folders = [\"books\",\"dvd\",\"electronics\",\"kitchen_&_housewares\",\"music\"]\n",
    "#x_train = list()\n",
    "#y_train = list()\n",
    "negative = 0\n",
    "positive = 1\n",
    "print('Reading Train Data')\n",
    "for folder in folders:\n",
    "  temp = open(path+folder+\"/negative.review\", 'r').read() # Read the file\n",
    "  temp = re.findall(regex_review, temp) # Get reviews\n",
    "  print(\"Reading\",len(temp),\"Negative reviews from\",folder)\n",
    "  #print(temp)\n",
    "  for sentence in temp:\n",
    "    list = []\n",
    "    sentences = clean_sentence(sentence)\n",
    "    sentences = ' '.join(map(str, sentences))\n",
    "    list.append(sentences)\n",
    "    list.append(negative)\n",
    "    Collections_test[0].append(list)\n",
    "    #print(Collections)\n",
    "\n",
    "\n",
    "  temp = open(path+folder+\"/positive.review\", 'r').read() # Read the file\n",
    "  temp = re.findall(regex_review, temp) # Get reviews\n",
    "  print(\"Reading\",len(temp),\"Positive reviews from\",folder)\n",
    "  for sentence in temp:\n",
    "    list = []\n",
    "    sentences = clean_sentence(sentence)\n",
    "    sentences = ' '.join(map(str, sentences))\n",
    "    list.append(sentences)\n",
    "    list.append(positive)\n",
    "    Collections_test[0].append(list)\n",
    "    #print(Collections) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddf = []\n",
    "#for key in Collections.keys():\n",
    "df = pd.DataFrame(Collections_test[0], columns = ['Review', 'Sentiment'])\n",
    "print(df.head(5))\n",
    "print(df.tail(5))\n",
    "    #ddf.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_df = df.sample(frac=1, random_state=1)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "split_index_1 = int(len(train_df) * 0.7)\n",
    "split_index_2 = int(len(train_df) * 0.85)\n",
    "\n",
    "train_dff, val_df, test_df = train_df[:7500], train_df[7500:9000], train_df[9000:]\n",
    "\n",
    "len(train_dff), len(val_df), len(test_df)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0cfdd3b0dffe2ad7fcbc64f5f93deae5239b8f311e6cb7a499fa34ebd238a891"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
