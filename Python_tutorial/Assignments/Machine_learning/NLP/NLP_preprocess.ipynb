{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abulf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abulf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abulf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\abulf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence: str) -> list:\n",
    "  # Remove the review tag of HTML\n",
    "  tags = re.compile(\"(<review_text>|<\\/review_text>)\")\n",
    "  sentence = re.sub(tags, '', sentence)\n",
    "  sentence = sentence.lower()\n",
    "  # Remove emails and urls\n",
    "  email_urls = re.compile(\"(\\bhttp.+? | \\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b)\")\n",
    "  sentence = re.sub(email_urls, '', sentence)\n",
    "  # Some used '@' to hide offensive words (bla -> bl@)\n",
    "  ats = re.compile('@')\n",
    "  sentence = re.sub(ats, 'a', sentence)\n",
    "  # Remove Punctuation \n",
    "  #punc = re.compile(\"[^\\w\\s(\\w+\\-\\w+)]\")\n",
    "  #punc = re.compile(\"[!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+,\\-\\.\\/\\:;<=>\\?\\[\\\\\\]\\^_`\\{\\|\\}\\~]\")\n",
    "  sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "  # Remove digits\n",
    "  pattern = r'[0-9]'\n",
    "  sentence = re.sub(pattern, '', sentence)\n",
    "  # Remove stopwords and tokenize\n",
    "  # sentence = sentence.split(sep=' ')\n",
    "  #sentence = word_tokenize(sentence)\n",
    "  #sentence = [word for word in sentence if not word in stopwords.words()]\n",
    "  #lemmatizer = WordNetLemmatizer()\n",
    "  #sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "  return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Train Data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/abulf/Documents/Python_tutorial/Assignments/Machine_learning/sorted_data_acl/books/negative.review'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\abulf\\Documents\\Python_tutorial\\Assignments\\Machine_learning\\NLP\\NLP_preprocess.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abulf/Documents/Python_tutorial/Assignments/Machine_learning/NLP/NLP_preprocess.ipynb#ch0000003?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mReading Train Data\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abulf/Documents/Python_tutorial/Assignments/Machine_learning/NLP/NLP_preprocess.ipynb#ch0000003?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m folder \u001b[39min\u001b[39;00m folders:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/abulf/Documents/Python_tutorial/Assignments/Machine_learning/NLP/NLP_preprocess.ipynb#ch0000003?line=12'>13</a>\u001b[0m   temp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(path\u001b[39m+\u001b[39;49mfolder\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/negative.review\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mread() \u001b[39m# Read the file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abulf/Documents/Python_tutorial/Assignments/Machine_learning/NLP/NLP_preprocess.ipynb#ch0000003?line=13'>14</a>\u001b[0m   temp \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mfindall(regex_review, temp) \u001b[39m# Get reviews\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abulf/Documents/Python_tutorial/Assignments/Machine_learning/NLP/NLP_preprocess.ipynb#ch0000003?line=14'>15</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mReading\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39mlen\u001b[39m(temp),\u001b[39m\"\u001b[39m\u001b[39mNegative reviews from\u001b[39m\u001b[39m\"\u001b[39m,folder)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/abulf/Documents/Python_tutorial/Assignments/Machine_learning/sorted_data_acl/books/negative.review'"
     ]
    }
   ],
   "source": [
    "path = \"/Users/abulf/Documents/Python_tutorial/Assignments/Machine_learning/sorted_data_acl/\"\n",
    "regex_review = re.compile(\"<review_text>.+?<\\/review_text>\", flags=re.DOTALL)\n",
    "\n",
    "Collections_test = defaultdict(list)\n",
    "# Training Data\n",
    "folders = [\"books\",\"dvd\",\"electronics\",\"kitchen_&_housewares\",\"music\"]\n",
    "#x_train = list()\n",
    "#y_train = list()\n",
    "negative = 0\n",
    "positive = 1\n",
    "print('Reading Train Data')\n",
    "for folder in folders:\n",
    "  temp = open(path+folder+\"/negative.review\", 'r').read() # Read the file\n",
    "  temp = re.findall(regex_review, temp) # Get reviews\n",
    "  print(\"Reading\",len(temp),\"Negative reviews from\",folder)\n",
    "  #print(temp)\n",
    "  for sentence in temp:\n",
    "    list = []\n",
    "    sentences = clean_sentence(sentence)\n",
    "    sentences = ' '.join(map(str, sentences))\n",
    "    list.append(sentences)\n",
    "    list.append(negative)\n",
    "    Collections_test[0].append(list)\n",
    "    #print(Collections)\n",
    "\n",
    "\n",
    "  temp = open(path+folder+\"/positive.review\", 'r').read() # Read the file\n",
    "  temp = re.findall(regex_review, temp) # Get reviews\n",
    "  print(\"Reading\",len(temp),\"Positive reviews from\",folder)\n",
    "  for sentence in temp:\n",
    "    list = []\n",
    "    sentences = clean_sentence(sentence)\n",
    "    sentences = ' '.join(map(str, sentences))\n",
    "    list.append(sentences)\n",
    "    list.append(positive)\n",
    "    Collections_test[0].append(list)\n",
    "    #print(Collections) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddf = []\n",
    "#for key in Collections.keys():\n",
    "df = pd.DataFrame(Collections_test[0], columns = ['Review', 'Sentiment'])\n",
    "print(df.head(5))\n",
    "print(df.tail(5))\n",
    "    #ddf.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_df = df.sample(frac=1, random_state=1)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "split_index_1 = int(len(train_df) * 0.7)\n",
    "split_index_2 = int(len(train_df) * 0.85)\n",
    "\n",
    "train_dff, val_df, test_df = train_df[:7500], train_df[7500:9000], train_df[9000:]\n",
    "\n",
    "len(train_dff), len(val_df), len(test_df)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('myvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f10a24a426163f4969834cd78cacbc8a505b0e39e0a52f8aaabad16331e335f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
