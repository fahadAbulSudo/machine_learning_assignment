{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#L is the size of total words in the vocab like my name is fahad that is 4 and d_k and d_v is same as d_total\n",
    "L, d_k, d_v = 4, 8, 8\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 1.09055468 -2.0516344   0.91705551 -0.80064762 -0.70158921 -0.8760401\n",
      "  -0.1307713   2.34696991]\n",
      " [ 0.43011261  0.45461376  0.45879596  0.54999023 -0.53638167 -0.25319405\n",
      "   0.24376715 -0.89314806]\n",
      " [ 0.45241956  2.64528065  0.50091089 -0.66364911 -0.19422367  0.72443613\n",
      "   0.95165321 -0.68233126]\n",
      " [-0.65959933 -1.69094675  0.14901582  0.56326842 -0.45650084  0.2401893\n",
      "  -0.5038496  -0.17789481]]\n",
      "K\n",
      " [[-0.12870307  0.76705569  0.34871384  0.39214776 -1.62535481  0.38578904\n",
      "   0.66376682 -1.41970566]\n",
      " [ 0.96666194 -0.68051125  0.86419713 -2.02803309  0.70481046  0.54318426\n",
      "  -0.90122704 -1.05106376]\n",
      " [-1.03823242 -0.30125604  0.29369403  0.34110531 -0.25871319 -0.5525009\n",
      "   1.08410506  0.18975028]\n",
      " [-0.31047456  0.69567149 -2.47769702 -0.7553853  -1.36809576  0.39458803\n",
      "  -0.71212114 -0.58026815]]\n",
      "V\n",
      " (4, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Q is query\"What I am looking for\"\n",
    "Here K is key \"What I can actually offer\"\n",
    "Here V is value \"What I am actually offering\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have four words my name is fahad so for \"My\" we have one key,query and value similarly for others "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.32470117,  1.54731559,  0.4511383 , -4.0878219 ],\n",
       "       [ 2.87296613, -0.40901695,  0.11229128, -0.39090134],\n",
       "       [ 4.08082312,  0.53211112, -0.793667  ,  1.22980007],\n",
       "       [-0.18655167, -0.05066708,  0.83554072, -0.58491404]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)#here we ar emultiplying the query vector with the key vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9289536360526895, 0.8258056671399958, 4.169402038505689)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why we need sqrt(d_k) in denominator because multiplication \"np.matmul(q, k.T)\" has variance \n",
    "q.var(), k.var(), np.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9289536360526895, 0.8258056671399958, 0.521175254813211)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see variance has decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.52901276,  0.54705867,  0.15950148, -1.44526329],\n",
       "       [ 1.01574691, -0.14460933,  0.03970096, -0.13820449],\n",
       "       [ 1.44278885,  0.18812969, -0.28060366,  0.43479998],\n",
       "       [-0.06595598, -0.01791352,  0.29540826, -0.20679834]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Masking\n",
    "This is to ensure words don't get context from words generated in the future.\n",
    "Not required in the encoders, but required in the decoders\n",
    "Because in encoders we push all corpus at one to bring out some contxt out of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones( (L, L) ))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.52901276,        -inf,        -inf,        -inf],\n",
       "       [ 1.01574691, -0.14460933,        -inf,        -inf],\n",
       "       [ 1.44278885,  0.18812969, -0.28060366,        -inf],\n",
       "       [-0.06595598, -0.01791352,  0.29540826, -0.20679834]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0\n",
    "scaled + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  #Softmax fuction is for probability distribution\n",
    "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.76139744, 0.23860256, 0.        , 0.        ],\n",
       "       [0.68323151, 0.1948392 , 0.12192929, 0.        ],\n",
       "       [0.22971989, 0.2410256 , 0.32971378, 0.19954072]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax(scaled + mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.93841437,  0.66362347,  0.56425023, -1.09439297, -0.03839097,\n",
       "        -0.68948602, -1.22307951,  1.56018408],\n",
       "       [ 1.06557988,  0.19574219,  0.54004458, -0.92956531, -0.01056279,\n",
       "        -0.39328143, -0.61874831,  0.99305028],\n",
       "       [ 1.24090607, -0.1198517 ,  0.6076907 , -0.85118665,  0.09242065,\n",
       "        -0.37267373, -0.41744399,  1.04859547],\n",
       "       [ 0.55792807, -1.20551915,  0.38610887, -0.30385878,  0.38441137,\n",
       "        -0.03254864,  0.42653801,  0.37863043]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.93841437,  0.66362347,  0.56425023, -1.09439297, -0.03839097,\n",
       "        -0.68948602, -1.22307951,  1.56018408],\n",
       "       [-1.71969595, -1.29729969,  0.46280264, -0.40358873,  0.07823885,\n",
       "         0.55192812,  1.30971481, -0.8167133 ],\n",
       "       [ 2.06336379, -2.62853452,  1.08263627, -0.20362604,  0.84808643,\n",
       "        -0.07489903,  1.33699368,  1.16261082],\n",
       "       [-0.76772795, -0.8951565 , -1.06253005,  0.56108139,  0.47482632,\n",
       "         0.08773371, -0.24554556, -0.83318772]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "  d_k = q.shape[-1]\n",
    "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    #For encoders it is none\n",
    "    scaled = scaled + mask\n",
    "  attention = softmax(scaled)\n",
    "  out = np.matmul(attention, v)\n",
    "  return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 1.09055468 -2.0516344   0.91705551 -0.80064762 -0.70158921 -0.8760401\n",
      "  -0.1307713   2.34696991]\n",
      " [ 0.43011261  0.45461376  0.45879596  0.54999023 -0.53638167 -0.25319405\n",
      "   0.24376715 -0.89314806]\n",
      " [ 0.45241956  2.64528065  0.50091089 -0.66364911 -0.19422367  0.72443613\n",
      "   0.95165321 -0.68233126]\n",
      " [-0.65959933 -1.69094675  0.14901582  0.56326842 -0.45650084  0.2401893\n",
      "  -0.5038496  -0.17789481]]\n",
      "K\n",
      " [[-0.12870307  0.76705569  0.34871384  0.39214776 -1.62535481  0.38578904\n",
      "   0.66376682 -1.41970566]\n",
      " [ 0.96666194 -0.68051125  0.86419713 -2.02803309  0.70481046  0.54318426\n",
      "  -0.90122704 -1.05106376]\n",
      " [-1.03823242 -0.30125604  0.29369403  0.34110531 -0.25871319 -0.5525009\n",
      "   1.08410506  0.18975028]\n",
      " [-0.31047456  0.69567149 -2.47769702 -0.7553853  -1.36809576  0.39458803\n",
      "  -0.71212114 -0.58026815]]\n",
      "V\n",
      " [[ 1.93841437  0.66362347  0.56425023 -1.09439297 -0.03839097 -0.68948602\n",
      "  -1.22307951  1.56018408]\n",
      " [-1.71969595 -1.29729969  0.46280264 -0.40358873  0.07823885  0.55192812\n",
      "   1.30971481 -0.8167133 ]\n",
      " [ 2.06336379 -2.62853452  1.08263627 -0.20362604  0.84808643 -0.07489903\n",
      "   1.33699368  1.16261082]\n",
      " [-0.76772795 -0.8951565  -1.06253005  0.56108139  0.47482632  0.08773371\n",
      "  -0.24554556 -0.83318772]]\n",
      "New V\n",
      " [[ 1.93841437  0.66362347  0.56425023 -1.09439297 -0.03839097 -0.68948602\n",
      "  -1.22307951  1.56018408]\n",
      " [ 1.06557988  0.19574219  0.54004458 -0.92956531 -0.01056279 -0.39328143\n",
      "  -0.61874831  0.99305028]\n",
      " [ 1.24090607 -0.1198517   0.6076907  -0.85118665  0.09242065 -0.37267373\n",
      "  -0.41744399  1.04859547]\n",
      " [ 0.55792807 -1.20551915  0.38610887 -0.30385878  0.38441137 -0.03254864\n",
      "   0.42653801  0.37863043]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.76139744 0.23860256 0.         0.        ]\n",
      " [0.68323151 0.1948392  0.12192929 0.        ]\n",
      " [0.22971989 0.2410256  0.32971378 0.19954072]]\n"
     ]
    }
   ],
   "source": [
    "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('myvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f10a24a426163f4969834cd78cacbc8a505b0e39e0a52f8aaabad16331e335f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
