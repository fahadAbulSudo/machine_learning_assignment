{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"x_final_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Standard Error</th>\n",
       "      <th>Median</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Sample Variance</th>\n",
       "      <th>Kurtosis</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Range</th>\n",
       "      <th>Minimum</th>\n",
       "      <th>Maximum</th>\n",
       "      <th>Sum</th>\n",
       "      <th>Count</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.142599</td>\n",
       "      <td>365.0</td>\n",
       "      <td>366</td>\n",
       "      <td>1.425985</td>\n",
       "      <td>2.033434</td>\n",
       "      <td>-0.897223</td>\n",
       "      <td>-0.105681</td>\n",
       "      <td>6</td>\n",
       "      <td>362</td>\n",
       "      <td>368</td>\n",
       "      <td>36513</td>\n",
       "      <td>100</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.126027</td>\n",
       "      <td>368.0</td>\n",
       "      <td>368</td>\n",
       "      <td>1.260271</td>\n",
       "      <td>1.588283</td>\n",
       "      <td>-0.395387</td>\n",
       "      <td>0.265678</td>\n",
       "      <td>5</td>\n",
       "      <td>366</td>\n",
       "      <td>371</td>\n",
       "      <td>36826</td>\n",
       "      <td>100</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.097333</td>\n",
       "      <td>371.0</td>\n",
       "      <td>371</td>\n",
       "      <td>0.973331</td>\n",
       "      <td>0.947374</td>\n",
       "      <td>-0.400559</td>\n",
       "      <td>-0.177908</td>\n",
       "      <td>4</td>\n",
       "      <td>369</td>\n",
       "      <td>373</td>\n",
       "      <td>37089</td>\n",
       "      <td>100</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.099372</td>\n",
       "      <td>373.0</td>\n",
       "      <td>373</td>\n",
       "      <td>0.993718</td>\n",
       "      <td>0.987475</td>\n",
       "      <td>-0.824843</td>\n",
       "      <td>0.072199</td>\n",
       "      <td>4</td>\n",
       "      <td>371</td>\n",
       "      <td>375</td>\n",
       "      <td>37332</td>\n",
       "      <td>100</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.094554</td>\n",
       "      <td>376.0</td>\n",
       "      <td>376</td>\n",
       "      <td>0.945537</td>\n",
       "      <td>0.894040</td>\n",
       "      <td>-0.187940</td>\n",
       "      <td>-0.205709</td>\n",
       "      <td>5</td>\n",
       "      <td>373</td>\n",
       "      <td>378</td>\n",
       "      <td>37557</td>\n",
       "      <td>100</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1.069559</td>\n",
       "      <td>325.0</td>\n",
       "      <td>330</td>\n",
       "      <td>10.695586</td>\n",
       "      <td>114.395556</td>\n",
       "      <td>0.435310</td>\n",
       "      <td>-0.668873</td>\n",
       "      <td>54</td>\n",
       "      <td>290</td>\n",
       "      <td>344</td>\n",
       "      <td>32322</td>\n",
       "      <td>100</td>\n",
       "      <td>Crater wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1.131819</td>\n",
       "      <td>324.5</td>\n",
       "      <td>327</td>\n",
       "      <td>11.318190</td>\n",
       "      <td>128.101414</td>\n",
       "      <td>-0.008641</td>\n",
       "      <td>-0.555331</td>\n",
       "      <td>53</td>\n",
       "      <td>291</td>\n",
       "      <td>344</td>\n",
       "      <td>32314</td>\n",
       "      <td>100</td>\n",
       "      <td>Crater wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1.137146</td>\n",
       "      <td>316.5</td>\n",
       "      <td>316</td>\n",
       "      <td>11.371464</td>\n",
       "      <td>129.310202</td>\n",
       "      <td>-0.046520</td>\n",
       "      <td>-0.069875</td>\n",
       "      <td>56</td>\n",
       "      <td>285</td>\n",
       "      <td>341</td>\n",
       "      <td>31677</td>\n",
       "      <td>100</td>\n",
       "      <td>Crater wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1.971869</td>\n",
       "      <td>344.5</td>\n",
       "      <td>346</td>\n",
       "      <td>19.718688</td>\n",
       "      <td>388.826667</td>\n",
       "      <td>-0.073315</td>\n",
       "      <td>-1.185157</td>\n",
       "      <td>74</td>\n",
       "      <td>283</td>\n",
       "      <td>357</td>\n",
       "      <td>33304</td>\n",
       "      <td>100</td>\n",
       "      <td>Crater wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.961634</td>\n",
       "      <td>347.5</td>\n",
       "      <td>348</td>\n",
       "      <td>3.846535</td>\n",
       "      <td>14.795833</td>\n",
       "      <td>2.440078</td>\n",
       "      <td>1.747884</td>\n",
       "      <td>13</td>\n",
       "      <td>345</td>\n",
       "      <td>358</td>\n",
       "      <td>5577</td>\n",
       "      <td>16</td>\n",
       "      <td>Crater wear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Standard Error  Median  Mode  Standard Deviation  Sample Variance  \\\n",
       "0          0.142599   365.0   366            1.425985         2.033434   \n",
       "1          0.126027   368.0   368            1.260271         1.588283   \n",
       "2          0.097333   371.0   371            0.973331         0.947374   \n",
       "3          0.099372   373.0   373            0.993718         0.987475   \n",
       "4          0.094554   376.0   376            0.945537         0.894040   \n",
       "..              ...     ...   ...                 ...              ...   \n",
       "295        1.069559   325.0   330           10.695586       114.395556   \n",
       "296        1.131819   324.5   327           11.318190       128.101414   \n",
       "297        1.137146   316.5   316           11.371464       129.310202   \n",
       "298        1.971869   344.5   346           19.718688       388.826667   \n",
       "299        0.961634   347.5   348            3.846535        14.795833   \n",
       "\n",
       "     Kurtosis  Skewness  Range  Minimum  Maximum    Sum  Count    Condition  \n",
       "0   -0.897223 -0.105681      6      362      368  36513    100         Good  \n",
       "1   -0.395387  0.265678      5      366      371  36826    100         Good  \n",
       "2   -0.400559 -0.177908      4      369      373  37089    100         Good  \n",
       "3   -0.824843  0.072199      4      371      375  37332    100         Good  \n",
       "4   -0.187940 -0.205709      5      373      378  37557    100         Good  \n",
       "..        ...       ...    ...      ...      ...    ...    ...          ...  \n",
       "295  0.435310 -0.668873     54      290      344  32322    100  Crater wear  \n",
       "296 -0.008641 -0.555331     53      291      344  32314    100  Crater wear  \n",
       "297 -0.046520 -0.069875     56      285      341  31677    100  Crater wear  \n",
       "298 -0.073315 -1.185157     74      283      357  33304    100  Crater wear  \n",
       "299  2.440078  1.747884     13      345      358   5577     16  Crater wear  \n",
       "\n",
       "[300 rows x 13 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dataset.iloc[:,0:12].values\n",
    "y=dataset.iloc[:,12].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "x=sc.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crater wear' 'Flank Wear' 'Good' 'Nose Wear' 'Notch wear'\n",
      " 'Tool breakage ']\n",
      "Crater wearFlank WearGoodNose WearNotch wearTool breakage \n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y))\n",
    "print(np.unique(y).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "y1=encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300, 12)\n"
     ]
    }
   ],
   "source": [
    "print(y1.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reshape:\n",
      " (300, 12, 1)\n",
      "300\n",
      "12\n",
      "1\n",
      "Sample shape:\n",
      " (12, 1)\n",
      "An example sample :\n",
      " [[-0.86939937]\n",
      " [ 1.40880986]\n",
      " [ 1.38417949]\n",
      " [-0.866907  ]\n",
      " [-0.45678934]\n",
      " [-0.39817647]\n",
      " [ 0.03912756]\n",
      " [-0.88567023]\n",
      " [ 1.25685037]\n",
      " [ 1.27250082]\n",
      " [ 1.29450258]\n",
      " [ 0.05783149]]\n"
     ]
    }
   ],
   "source": [
    "sample_size=x.shape[0]\n",
    "time_steps=x.shape[1]\n",
    "input_dimension=1\n",
    "x_reshaped=x.reshape(sample_size,time_steps,input_dimension)\n",
    "\n",
    "print(\"After reshape:\\n\",x_reshaped.shape)\n",
    "print(x_reshaped.shape[0])\n",
    "print(x_reshaped.shape[1])\n",
    "print(x_reshaped.shape[2])\n",
    "print(\"Sample shape:\\n\",x_reshaped[0].shape)\n",
    "print(\"An example sample :\\n\",x_reshaped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "[[[ -0.41412406]\n",
      "  [  0.56211278]\n",
      "  [  0.53269112]\n",
      "  ...\n",
      "  [  0.04123755]\n",
      "  [  0.50915787]\n",
      "  [  0.05783149]]\n",
      "\n",
      " [[  1.52368499]\n",
      "  [ -0.84904901]\n",
      "  [  0.39645298]\n",
      "  ...\n",
      "  [  0.27576389]\n",
      "  [ -0.59737909]\n",
      "  [  0.05783149]]\n",
      "\n",
      " [[  0.71675509]\n",
      "  [ -0.7608514 ]\n",
      "  [ -1.10216654]\n",
      "  ...\n",
      "  [  0.56892181]\n",
      "  [ -0.59925193]\n",
      "  [  0.05783149]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  0.12883512]\n",
      "  [ -0.30222382]\n",
      "  [ -0.31879724]\n",
      "  ...\n",
      "  [ -0.31055196]\n",
      "  [ -0.21500736]\n",
      "  [  0.05783149]]\n",
      "\n",
      " [[  0.96665167]\n",
      "  [  0.68558944]\n",
      "  [  0.70298879]\n",
      "  ...\n",
      "  [  0.62755339]\n",
      "  [  0.29284472]\n",
      "  [  0.05783149]]\n",
      "\n",
      " [[ -0.0473281 ]\n",
      "  [  0.79142657]\n",
      "  [  0.77110786]\n",
      "  ...\n",
      "  [  0.68618498]\n",
      "  [ -8.36186624]\n",
      "  [-17.29161647]]] having: 200\n",
      "[[[-0.86939937]\n",
      "  [ 1.40880986]\n",
      "  [ 1.38417949]\n",
      "  ...\n",
      "  [ 1.27250082]\n",
      "  [ 1.29450258]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " [[-0.88603225]\n",
      "  [ 1.51464699]\n",
      "  [ 1.45229855]\n",
      "  ...\n",
      "  [ 1.44839558]\n",
      "  [ 1.39220246]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " [[-0.91483258]\n",
      "  [ 1.62048413]\n",
      "  [ 1.55447716]\n",
      "  ...\n",
      "  [ 1.56565875]\n",
      "  [ 1.47429533]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.92612546]\n",
      "  [ 0.8090661 ]\n",
      "  [ 0.77110786]\n",
      "  ...\n",
      "  [ 0.2171323 ]\n",
      "  [ 0.75730932]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " [[-0.93989554]\n",
      "  [ 0.8090661 ]\n",
      "  [ 0.77110786]\n",
      "  ...\n",
      "  [ 0.2171323 ]\n",
      "  [ 0.75855788]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " [[-0.93615972]\n",
      "  [ 0.8090661 ]\n",
      "  [ 0.77110786]\n",
      "  ...\n",
      "  [ 0.2171323 ]\n",
      "  [ 0.75574862]\n",
      "  [ 0.05783149]]] having: 100\n",
      "--------------------------\n",
      "fold 2\n",
      "[[[ -0.86939937]\n",
      "  [  1.40880986]\n",
      "  [  1.38417949]\n",
      "  ...\n",
      "  [  1.27250082]\n",
      "  [  1.29450258]\n",
      "  [  0.05783149]]\n",
      "\n",
      " [[ -0.88603225]\n",
      "  [  1.51464699]\n",
      "  [  1.45229855]\n",
      "  ...\n",
      "  [  1.44839558]\n",
      "  [  1.39220246]\n",
      "  [  0.05783149]]\n",
      "\n",
      " [[ -0.91483258]\n",
      "  [  1.62048413]\n",
      "  [  1.55447716]\n",
      "  ...\n",
      "  [  1.56565875]\n",
      "  [  1.47429533]\n",
      "  [  0.05783149]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  0.12883512]\n",
      "  [ -0.30222382]\n",
      "  [ -0.31879724]\n",
      "  ...\n",
      "  [ -0.31055196]\n",
      "  [ -0.21500736]\n",
      "  [  0.05783149]]\n",
      "\n",
      " [[  0.96665167]\n",
      "  [  0.68558944]\n",
      "  [  0.70298879]\n",
      "  ...\n",
      "  [  0.62755339]\n",
      "  [  0.29284472]\n",
      "  [  0.05783149]]\n",
      "\n",
      " [[ -0.0473281 ]\n",
      "  [  0.79142657]\n",
      "  [  0.77110786]\n",
      "  ...\n",
      "  [  0.68618498]\n",
      "  [ -8.36186624]\n",
      "  [-17.29161647]]] having: 200\n",
      "[[[-0.41412406]\n",
      "  [ 0.56211278]\n",
      "  [ 0.53269112]\n",
      "  ...\n",
      "  [ 0.04123755]\n",
      "  [ 0.50915787]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " [[ 1.52368499]\n",
      "  [-0.84904901]\n",
      "  [ 0.39645298]\n",
      "  ...\n",
      "  [ 0.27576389]\n",
      "  [-0.59737909]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " [[ 0.71675509]\n",
      "  [-0.7608514 ]\n",
      "  [-1.10216654]\n",
      "  ...\n",
      "  [ 0.56892181]\n",
      "  [-0.59925193]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.11512288]\n",
      "  [ 0.61503135]\n",
      "  [ 0.63486972]\n",
      "  ...\n",
      "  [ 0.9793429 ]\n",
      "  [ 0.51071857]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " [[ 0.22332012]\n",
      "  [ 0.61503135]\n",
      "  [ 0.70298879]\n",
      "  ...\n",
      "  [ 0.92071132]\n",
      "  [ 0.52632558]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " [[ 0.22233988]\n",
      "  [ 0.59739183]\n",
      "  [ 0.83922693]\n",
      "  ...\n",
      "  [ 0.86207973]\n",
      "  [ 0.50322721]\n",
      "  [ 0.05783149]]] having: 100\n",
      "--------------------------\n",
      "fold 3\n",
      "[[[-0.86939937]\n",
      "  [ 1.40880986]\n",
      "  [ 1.38417949]\n",
      "  ...\n",
      "  [ 1.27250082]\n",
      "  [ 1.29450258]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " [[-0.88603225]\n",
      "  [ 1.51464699]\n",
      "  [ 1.45229855]\n",
      "  ...\n",
      "  [ 1.44839558]\n",
      "  [ 1.39220246]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " [[-0.91483258]\n",
      "  [ 1.62048413]\n",
      "  [ 1.55447716]\n",
      "  ...\n",
      "  [ 1.56565875]\n",
      "  [ 1.47429533]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.11512288]\n",
      "  [ 0.61503135]\n",
      "  [ 0.63486972]\n",
      "  ...\n",
      "  [ 0.9793429 ]\n",
      "  [ 0.51071857]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " [[ 0.22332012]\n",
      "  [ 0.61503135]\n",
      "  [ 0.70298879]\n",
      "  ...\n",
      "  [ 0.92071132]\n",
      "  [ 0.52632558]\n",
      "  [ 0.05783149]]\n",
      "\n",
      " [[ 0.22233988]\n",
      "  [ 0.59739183]\n",
      "  [ 0.83922693]\n",
      "  ...\n",
      "  [ 0.86207973]\n",
      "  [ 0.50322721]\n",
      "  [ 0.05783149]]] having: 200\n",
      "[[[ -0.79140015]\n",
      "  [  0.66794992]\n",
      "  [  0.66892926]\n",
      "  ...\n",
      "  [  0.15850072]\n",
      "  [  0.62152834]\n",
      "  [  0.05783149]]\n",
      "\n",
      " [[ -0.71698123]\n",
      "  [  0.66794992]\n",
      "  [  0.60081019]\n",
      "  ...\n",
      "  [  0.27576389]\n",
      "  [  0.62184048]\n",
      "  [  0.05783149]]\n",
      "\n",
      " [[ -0.6571167 ]\n",
      "  [  0.66794992]\n",
      "  [  0.63486972]\n",
      "  ...\n",
      "  [  0.33439547]\n",
      "  [  0.63900819]\n",
      "  [  0.05783149]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  0.12883512]\n",
      "  [ -0.30222382]\n",
      "  [ -0.31879724]\n",
      "  ...\n",
      "  [ -0.31055196]\n",
      "  [ -0.21500736]\n",
      "  [  0.05783149]]\n",
      "\n",
      " [[  0.96665167]\n",
      "  [  0.68558944]\n",
      "  [  0.70298879]\n",
      "  ...\n",
      "  [  0.62755339]\n",
      "  [  0.29284472]\n",
      "  [  0.05783149]]\n",
      "\n",
      " [[ -0.0473281 ]\n",
      "  [  0.79142657]\n",
      "  [  0.77110786]\n",
      "  ...\n",
      "  [  0.68618498]\n",
      "  [ -8.36186624]\n",
      "  [-17.29161647]]] having: 100\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=3,shuffle=False,random_state=None)\n",
    "\n",
    "i=1\n",
    "for train_index,test_index in kf.split(x_reshaped,y1):\n",
    "    print(\"fold\",i)\n",
    "    x_train,x_test=x_reshaped[train_index],x_reshaped[test_index]\n",
    "    y_train,y_test=y1[train_index],y1[test_index]\n",
    "    print(x_train,\"having:\",len(x_train))\n",
    "    print(x_test,\"having:\",len(x_test))\n",
    "    print(\"--------------------------\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y1))\n",
    "print(np.unique(y1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "model=keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv1D(filters=64, kernel_size=2, strides=1,padding=\"valid\",activation='relu',use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\", input_shape=(12,1)))\n",
    " \n",
    "model.add(keras.layers.Conv1D(filters=32, kernel_size=2, strides=1,padding=\"valid\",activation='relu',use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\", input_shape=(12,1)))\n",
    "    \n",
    "\n",
    "model.add(keras.layers.MaxPooling1D(2,1))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(6,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "               loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.0797 - accuracy: 0.9701 - val_loss: 26.1049 - val_accuracy: 0.2424\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0882 - accuracy: 0.9627 - val_loss: 26.0383 - val_accuracy: 0.2424\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1177 - accuracy: 0.97 - 0s 13ms/step - loss: 0.1177 - accuracy: 0.9701 - val_loss: 25.8607 - val_accuracy: 0.2424\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0810 - accuracy: 0.9627 - val_loss: 25.6569 - val_accuracy: 0.2424\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0680 - accuracy: 0.9701 - val_loss: 25.5615 - val_accuracy: 0.2424\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0699 - accuracy: 0.9701 - val_loss: 25.5992 - val_accuracy: 0.2424\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0608 - accuracy: 0.9776 - val_loss: 25.7492 - val_accuracy: 0.2424\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0700 - accuracy: 0.9776 - val_loss: 25.8475 - val_accuracy: 0.2424\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0639 - accuracy: 0.9776 - val_loss: 25.8934 - val_accuracy: 0.2424\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.0680 - accuracy: 0.9851 - val_loss: 25.9720 - val_accuracy: 0.2424\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0656 - accuracy: 0.9701 - val_loss: 26.1011 - val_accuracy: 0.2424\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0706 - accuracy: 0.9627 - val_loss: 26.1255 - val_accuracy: 0.2424\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0710 - accuracy: 0.9776 - val_loss: 25.9171 - val_accuracy: 0.2424\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0777 - accuracy: 0.9627 - val_loss: 26.0483 - val_accuracy: 0.2424\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0620 - accuracy: 0.9701 - val_loss: 25.9375 - val_accuracy: 0.2424\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0710 - accuracy: 0.9776 - val_loss: 26.0495 - val_accuracy: 0.2424\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0721 - accuracy: 0.9627 - val_loss: 26.1897 - val_accuracy: 0.2424\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0662 - accuracy: 0.9776 - val_loss: 26.1993 - val_accuracy: 0.2424\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0635 - accuracy: 0.9776 - val_loss: 26.2978 - val_accuracy: 0.2424\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0730 - accuracy: 0.9552 - val_loss: 26.2525 - val_accuracy: 0.2424\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0707 - accuracy: 0.9776 - val_loss: 26.2684 - val_accuracy: 0.2424\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.0776 - accuracy: 0.9627 - val_loss: 26.3201 - val_accuracy: 0.2424\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0822 - accuracy: 0.9701 - val_loss: 26.2069 - val_accuracy: 0.2424\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0664 - accuracy: 0.9627 - val_loss: 26.2537 - val_accuracy: 0.2424\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0682 - accuracy: 0.9627 - val_loss: 26.1713 - val_accuracy: 0.2424\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0721 - accuracy: 0.9701 - val_loss: 26.2606 - val_accuracy: 0.2424\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0611 - accuracy: 0.9776 - val_loss: 26.2175 - val_accuracy: 0.2424\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0713 - accuracy: 0.9701 - val_loss: 26.3779 - val_accuracy: 0.2424\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0702 - accuracy: 0.9701 - val_loss: 26.4253 - val_accuracy: 0.2424\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0670 - accuracy: 0.9627 - val_loss: 26.4314 - val_accuracy: 0.2424\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0698 - accuracy: 0.9776 - val_loss: 26.3689 - val_accuracy: 0.2424\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0623 - accuracy: 0.9776 - val_loss: 26.4397 - val_accuracy: 0.2424\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0671 - accuracy: 0.9776 - val_loss: 26.5566 - val_accuracy: 0.2424\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.0659 - accuracy: 0.9776 - val_loss: 26.5681 - val_accuracy: 0.2424\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0761 - accuracy: 0.9701 - val_loss: 26.4876 - val_accuracy: 0.2424\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.0878 - accuracy: 0.9701 - val_loss: 26.3608 - val_accuracy: 0.2424\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0608 - accuracy: 0.9776 - val_loss: 26.4398 - val_accuracy: 0.2424\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.0748 - accuracy: 0.9552 - val_loss: 26.5180 - val_accuracy: 0.2424\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.1354 - accuracy: 0.9254 - val_loss: 25.9539 - val_accuracy: 0.2424\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0846 - accuracy: 0.9627 - val_loss: 25.7839 - val_accuracy: 0.2424\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0629 - accuracy: 0.9776 - val_loss: 25.8813 - val_accuracy: 0.2424\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0730 - accuracy: 0.9701 - val_loss: 25.9744 - val_accuracy: 0.2424\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.1021 - accuracy: 0.9403 - val_loss: 26.0245 - val_accuracy: 0.2424\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.1183 - accuracy: 0.9627 - val_loss: 25.8098 - val_accuracy: 0.2424\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0816 - accuracy: 0.9627 - val_loss: 25.6864 - val_accuracy: 0.2424\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0637 - accuracy: 0.9627 - val_loss: 25.4464 - val_accuracy: 0.2424\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.0600 - accuracy: 0.9776 - val_loss: 25.5783 - val_accuracy: 0.2424\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0664 - accuracy: 0.9776 - val_loss: 25.6788 - val_accuracy: 0.2424\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.0629 - accuracy: 0.9776 - val_loss: 25.8164 - val_accuracy: 0.2424\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0614 - accuracy: 0.9776 - val_loss: 25.8985 - val_accuracy: 0.2424\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0633 - accuracy: 0.9776 - val_loss: 25.9809 - val_accuracy: 0.2424\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0608 - accuracy: 0.9776 - val_loss: 25.9803 - val_accuracy: 0.2424\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0620 - accuracy: 0.9776 - val_loss: 26.0646 - val_accuracy: 0.2424\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0606 - accuracy: 0.9701 - val_loss: 26.0881 - val_accuracy: 0.2424\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0671 - accuracy: 0.9701 - val_loss: 26.1170 - val_accuracy: 0.2424\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.0791 - accuracy: 0.9627 - val_loss: 26.0352 - val_accuracy: 0.2424\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.0752 - accuracy: 0.9701 - val_loss: 25.9082 - val_accuracy: 0.2424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 0.0744 - accuracy: 0.9776 - val_loss: 25.9052 - val_accuracy: 0.2424\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0964 - accuracy: 0.9627 - val_loss: 25.9026 - val_accuracy: 0.2424\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0592 - accuracy: 0.9776 - val_loss: 26.0153 - val_accuracy: 0.2424\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0592 - accuracy: 0.9776 - val_loss: 26.1770 - val_accuracy: 0.2424\n",
      "Epoch 62/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0567 - accuracy: 0.9851 - val_loss: 26.3549 - val_accuracy: 0.2424\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0678 - accuracy: 0.9776 - val_loss: 26.3148 - val_accuracy: 0.2424\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0579 - accuracy: 0.9851 - val_loss: 26.3207 - val_accuracy: 0.2424\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.0679 - accuracy: 0.9776 - val_loss: 26.3428 - val_accuracy: 0.2424\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.0723 - accuracy: 0.9627 - val_loss: 26.5556 - val_accuracy: 0.2424\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.0601 - accuracy: 0.9701 - val_loss: 26.5182 - val_accuracy: 0.2424\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0584 - accuracy: 0.9776 - val_loss: 26.6682 - val_accuracy: 0.2424\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0553 - accuracy: 0.9776 - val_loss: 26.6749 - val_accuracy: 0.2424\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.0599 - accuracy: 0.9851 - val_loss: 26.7984 - val_accuracy: 0.2424\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0754 - accuracy: 0.9627 - val_loss: 26.7380 - val_accuracy: 0.2424\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0653 - accuracy: 0.9776 - val_loss: 26.5937 - val_accuracy: 0.2424\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0577 - accuracy: 0.9776 - val_loss: 26.7041 - val_accuracy: 0.2424\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.0674 - accuracy: 0.9701 - val_loss: 26.7528 - val_accuracy: 0.2424\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0600 - accuracy: 0.9776 - val_loss: 26.8840 - val_accuracy: 0.2424\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0732 - accuracy: 0.9776 - val_loss: 26.8327 - val_accuracy: 0.2424\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0630 - accuracy: 0.9627 - val_loss: 26.9417 - val_accuracy: 0.2424\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0613 - accuracy: 0.9776 - val_loss: 26.7740 - val_accuracy: 0.2424\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0587 - accuracy: 0.9776 - val_loss: 26.8619 - val_accuracy: 0.2424\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0571 - accuracy: 0.9776 - val_loss: 26.9836 - val_accuracy: 0.2424\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0629 - accuracy: 0.9776 - val_loss: 27.0366 - val_accuracy: 0.2424\n",
      "Epoch 82/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0614 - accuracy: 0.9776 - val_loss: 27.1500 - val_accuracy: 0.2424\n",
      "Epoch 83/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0867 - accuracy: 0.9701 - val_loss: 27.1056 - val_accuracy: 0.2424\n",
      "Epoch 84/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0654 - accuracy: 0.9701 - val_loss: 26.9771 - val_accuracy: 0.2424\n",
      "Epoch 85/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0771 - accuracy: 0.9701 - val_loss: 27.0452 - val_accuracy: 0.2424\n",
      "Epoch 86/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0706 - accuracy: 0.9776 - val_loss: 26.9516 - val_accuracy: 0.2424\n",
      "Epoch 87/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.0647 - accuracy: 0.9851 - val_loss: 27.0530 - val_accuracy: 0.2424\n",
      "Epoch 88/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0576 - accuracy: 0.9776 - val_loss: 27.0994 - val_accuracy: 0.2424\n",
      "Epoch 89/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0707 - accuracy: 0.9776 - val_loss: 27.1590 - val_accuracy: 0.2424\n",
      "Epoch 90/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0717 - accuracy: 0.9701 - val_loss: 27.0473 - val_accuracy: 0.2424\n",
      "Epoch 91/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0478 - accuracy: 0.9851 - val_loss: 27.2750 - val_accuracy: 0.2424\n",
      "Epoch 92/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0603 - accuracy: 0.9776 - val_loss: 27.1047 - val_accuracy: 0.2424\n",
      "Epoch 93/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0606 - accuracy: 0.9701 - val_loss: 27.2278 - val_accuracy: 0.2424\n",
      "Epoch 94/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0593 - accuracy: 0.9776 - val_loss: 27.3482 - val_accuracy: 0.2424\n",
      "Epoch 95/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0583 - accuracy: 0.9776 - val_loss: 27.2743 - val_accuracy: 0.2424\n",
      "Epoch 96/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0732 - accuracy: 0.9552 - val_loss: 27.2681 - val_accuracy: 0.2424\n",
      "Epoch 97/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0570 - accuracy: 0.9776 - val_loss: 27.3165 - val_accuracy: 0.2424\n",
      "Epoch 98/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.0681 - accuracy: 0.9701 - val_loss: 27.4830 - val_accuracy: 0.2424\n",
      "Epoch 99/100\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.0589 - accuracy: 0.9776 - val_loss: 27.3743 - val_accuracy: 0.2424\n",
      "Epoch 100/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.0557 - accuracy: 0.9851 - val_loss: 27.4313 - val_accuracy: 0.2424\n"
     ]
    }
   ],
   "source": [
    "model_history=model.fit(x_train, y_train,validation_split=.33,batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.pred=model.predict(x_test)\n",
    "y_pred=np.pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      50.0\n",
      "           2       0.00      0.00      0.00       0.0\n",
      "           3       0.00      0.00      0.00       0.0\n",
      "           4       0.00      0.00      0.00       0.0\n",
      "           5       0.00      0.00      0.00      50.0\n",
      "\n",
      "    accuracy                           0.00     100.0\n",
      "   macro avg       0.00      0.00      0.00     100.0\n",
      "weighted avg       0.00      0.00      0.00     100.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(69.0, 0.5, 'Truth')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "cm=tf.math.confusion_matrix(labels=y_test,predictions=y_pred)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.heatmap(cm,annot=True,fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8681318681318682"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy\n",
    "acc=(12+11+10+14+13+19)/(14+15+13+15+14+20)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(acc,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
