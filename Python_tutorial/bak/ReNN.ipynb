{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Standard Error</th>\n",
       "      <th>Median</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Sample Variance</th>\n",
       "      <th>Kurtosis</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Range</th>\n",
       "      <th>Minimum</th>\n",
       "      <th>Maximum</th>\n",
       "      <th>Sum</th>\n",
       "      <th>Count</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.142599</td>\n",
       "      <td>365.0</td>\n",
       "      <td>366</td>\n",
       "      <td>1.425985</td>\n",
       "      <td>2.033434</td>\n",
       "      <td>-0.897223</td>\n",
       "      <td>-0.105681</td>\n",
       "      <td>6</td>\n",
       "      <td>362</td>\n",
       "      <td>368</td>\n",
       "      <td>36513</td>\n",
       "      <td>100</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.126027</td>\n",
       "      <td>368.0</td>\n",
       "      <td>368</td>\n",
       "      <td>1.260271</td>\n",
       "      <td>1.588283</td>\n",
       "      <td>-0.395387</td>\n",
       "      <td>0.265678</td>\n",
       "      <td>5</td>\n",
       "      <td>366</td>\n",
       "      <td>371</td>\n",
       "      <td>36826</td>\n",
       "      <td>100</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.097333</td>\n",
       "      <td>371.0</td>\n",
       "      <td>371</td>\n",
       "      <td>0.973331</td>\n",
       "      <td>0.947374</td>\n",
       "      <td>-0.400559</td>\n",
       "      <td>-0.177908</td>\n",
       "      <td>4</td>\n",
       "      <td>369</td>\n",
       "      <td>373</td>\n",
       "      <td>37089</td>\n",
       "      <td>100</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.099372</td>\n",
       "      <td>373.0</td>\n",
       "      <td>373</td>\n",
       "      <td>0.993718</td>\n",
       "      <td>0.987475</td>\n",
       "      <td>-0.824843</td>\n",
       "      <td>0.072199</td>\n",
       "      <td>4</td>\n",
       "      <td>371</td>\n",
       "      <td>375</td>\n",
       "      <td>37332</td>\n",
       "      <td>100</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.094554</td>\n",
       "      <td>376.0</td>\n",
       "      <td>376</td>\n",
       "      <td>0.945537</td>\n",
       "      <td>0.894040</td>\n",
       "      <td>-0.187940</td>\n",
       "      <td>-0.205709</td>\n",
       "      <td>5</td>\n",
       "      <td>373</td>\n",
       "      <td>378</td>\n",
       "      <td>37557</td>\n",
       "      <td>100</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1.069559</td>\n",
       "      <td>325.0</td>\n",
       "      <td>330</td>\n",
       "      <td>10.695586</td>\n",
       "      <td>114.395556</td>\n",
       "      <td>0.435310</td>\n",
       "      <td>-0.668873</td>\n",
       "      <td>54</td>\n",
       "      <td>290</td>\n",
       "      <td>344</td>\n",
       "      <td>32322</td>\n",
       "      <td>100</td>\n",
       "      <td>Crater wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1.131819</td>\n",
       "      <td>324.5</td>\n",
       "      <td>327</td>\n",
       "      <td>11.318190</td>\n",
       "      <td>128.101414</td>\n",
       "      <td>-0.008641</td>\n",
       "      <td>-0.555331</td>\n",
       "      <td>53</td>\n",
       "      <td>291</td>\n",
       "      <td>344</td>\n",
       "      <td>32314</td>\n",
       "      <td>100</td>\n",
       "      <td>Crater wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1.137146</td>\n",
       "      <td>316.5</td>\n",
       "      <td>316</td>\n",
       "      <td>11.371464</td>\n",
       "      <td>129.310202</td>\n",
       "      <td>-0.046520</td>\n",
       "      <td>-0.069875</td>\n",
       "      <td>56</td>\n",
       "      <td>285</td>\n",
       "      <td>341</td>\n",
       "      <td>31677</td>\n",
       "      <td>100</td>\n",
       "      <td>Crater wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1.971869</td>\n",
       "      <td>344.5</td>\n",
       "      <td>346</td>\n",
       "      <td>19.718688</td>\n",
       "      <td>388.826667</td>\n",
       "      <td>-0.073315</td>\n",
       "      <td>-1.185157</td>\n",
       "      <td>74</td>\n",
       "      <td>283</td>\n",
       "      <td>357</td>\n",
       "      <td>33304</td>\n",
       "      <td>100</td>\n",
       "      <td>Crater wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.961634</td>\n",
       "      <td>347.5</td>\n",
       "      <td>348</td>\n",
       "      <td>3.846535</td>\n",
       "      <td>14.795833</td>\n",
       "      <td>2.440078</td>\n",
       "      <td>1.747884</td>\n",
       "      <td>13</td>\n",
       "      <td>345</td>\n",
       "      <td>358</td>\n",
       "      <td>5577</td>\n",
       "      <td>16</td>\n",
       "      <td>Crater wear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Standard Error  Median  Mode  Standard Deviation  Sample Variance  \\\n",
       "0          0.142599   365.0   366            1.425985         2.033434   \n",
       "1          0.126027   368.0   368            1.260271         1.588283   \n",
       "2          0.097333   371.0   371            0.973331         0.947374   \n",
       "3          0.099372   373.0   373            0.993718         0.987475   \n",
       "4          0.094554   376.0   376            0.945537         0.894040   \n",
       "..              ...     ...   ...                 ...              ...   \n",
       "295        1.069559   325.0   330           10.695586       114.395556   \n",
       "296        1.131819   324.5   327           11.318190       128.101414   \n",
       "297        1.137146   316.5   316           11.371464       129.310202   \n",
       "298        1.971869   344.5   346           19.718688       388.826667   \n",
       "299        0.961634   347.5   348            3.846535        14.795833   \n",
       "\n",
       "     Kurtosis  Skewness  Range  Minimum  Maximum    Sum  Count    Condition  \n",
       "0   -0.897223 -0.105681      6      362      368  36513    100         Good  \n",
       "1   -0.395387  0.265678      5      366      371  36826    100         Good  \n",
       "2   -0.400559 -0.177908      4      369      373  37089    100         Good  \n",
       "3   -0.824843  0.072199      4      371      375  37332    100         Good  \n",
       "4   -0.187940 -0.205709      5      373      378  37557    100         Good  \n",
       "..        ...       ...    ...      ...      ...    ...    ...          ...  \n",
       "295  0.435310 -0.668873     54      290      344  32322    100  Crater wear  \n",
       "296 -0.008641 -0.555331     53      291      344  32314    100  Crater wear  \n",
       "297 -0.046520 -0.069875     56      285      341  31677    100  Crater wear  \n",
       "298 -0.073315 -1.185157     74      283      357  33304    100  Crater wear  \n",
       "299  2.440078  1.747884     13      345      358   5577     16  Crater wear  \n",
       "\n",
       "[300 rows x 13 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.read_csv(\"x_final_csv.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dataset.iloc[:,0:12].values\n",
    "y=dataset.iloc[:,12].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "x=sc.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crater wear' 'Flank Wear' 'Good' 'Nose Wear' 'Notch wear'\n",
      " 'Tool breakage ']\n",
      "Crater wearFlank WearGoodNose WearNotch wearTool breakage \n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y))\n",
    "print(np.unique(y).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "y1=encoder.fit_transform(y)\n",
    "y2=pd.get_dummies(y1).values\n",
    "print(y1.shape)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reshape:\n",
      " (300, 12, 1)\n",
      "Sample shape:\n",
      " (12, 1)\n",
      "An example sample :\n",
      " [[-0.86939937]\n",
      " [ 1.40880986]\n",
      " [ 1.38417949]\n",
      " [-0.866907  ]\n",
      " [-0.45678934]\n",
      " [-0.39817647]\n",
      " [ 0.03912756]\n",
      " [-0.88567023]\n",
      " [ 1.25685037]\n",
      " [ 1.27250082]\n",
      " [ 1.29450258]\n",
      " [ 0.05783149]]\n"
     ]
    }
   ],
   "source": [
    "sample_size=x.shape[0]\n",
    "time_steps=x.shape[1]\n",
    "input_dimension=1\n",
    "x_reshaped=x.reshape(sample_size,time_steps,input_dimension)\n",
    "\n",
    "print(\"After reshape:\\n\",x_reshaped.shape)\n",
    "print(\"Sample shape:\\n\",x_reshaped[0].shape)\n",
    "print(\"An example sample :\\n\",x_reshaped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=3,shuffle=False,random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "[[ -0.41412406   0.56211278   0.53269112 ...   0.04123755   0.50915787\n",
      "    0.05783149]\n",
      " [  1.52368499  -0.84904901   0.39645298 ...   0.27576389  -0.59737909\n",
      "    0.05783149]\n",
      " [  0.71675509  -0.7608514   -1.10216654 ...   0.56892181  -0.59925193\n",
      "    0.05783149]\n",
      " ...\n",
      " [  0.12883512  -0.30222382  -0.31879724 ...  -0.31055196  -0.21500736\n",
      "    0.05783149]\n",
      " [  0.96665167   0.68558944   0.70298879 ...   0.62755339   0.29284472\n",
      "    0.05783149]\n",
      " [ -0.0473281    0.79142657   0.77110786 ...   0.68618498  -8.36186624\n",
      "  -17.29161647]] having: 200\n",
      "[[-0.86939937  1.40880986  1.38417949 ...  1.27250082  1.29450258\n",
      "   0.05783149]\n",
      " [-0.88603225  1.51464699  1.45229855 ...  1.44839558  1.39220246\n",
      "   0.05783149]\n",
      " [-0.91483258  1.62048413  1.55447716 ...  1.56565875  1.47429533\n",
      "   0.05783149]\n",
      " ...\n",
      " [-0.92612546  0.8090661   0.77110786 ...  0.2171323   0.75730932\n",
      "   0.05783149]\n",
      " [-0.93989554  0.8090661   0.77110786 ...  0.2171323   0.75855788\n",
      "   0.05783149]\n",
      " [-0.93615972  0.8090661   0.77110786 ...  0.2171323   0.75574862\n",
      "   0.05783149]] having: 100\n",
      "--------------------------\n",
      "fold 2\n",
      "[[ -0.86939937   1.40880986   1.38417949 ...   1.27250082   1.29450258\n",
      "    0.05783149]\n",
      " [ -0.88603225   1.51464699   1.45229855 ...   1.44839558   1.39220246\n",
      "    0.05783149]\n",
      " [ -0.91483258   1.62048413   1.55447716 ...   1.56565875   1.47429533\n",
      "    0.05783149]\n",
      " ...\n",
      " [  0.12883512  -0.30222382  -0.31879724 ...  -0.31055196  -0.21500736\n",
      "    0.05783149]\n",
      " [  0.96665167   0.68558944   0.70298879 ...   0.62755339   0.29284472\n",
      "    0.05783149]\n",
      " [ -0.0473281    0.79142657   0.77110786 ...   0.68618498  -8.36186624\n",
      "  -17.29161647]] having: 200\n",
      "[[-0.41412406  0.56211278  0.53269112 ...  0.04123755  0.50915787\n",
      "   0.05783149]\n",
      " [ 1.52368499 -0.84904901  0.39645298 ...  0.27576389 -0.59737909\n",
      "   0.05783149]\n",
      " [ 0.71675509 -0.7608514  -1.10216654 ...  0.56892181 -0.59925193\n",
      "   0.05783149]\n",
      " ...\n",
      " [ 0.11512288  0.61503135  0.63486972 ...  0.9793429   0.51071857\n",
      "   0.05783149]\n",
      " [ 0.22332012  0.61503135  0.70298879 ...  0.92071132  0.52632558\n",
      "   0.05783149]\n",
      " [ 0.22233988  0.59739183  0.83922693 ...  0.86207973  0.50322721\n",
      "   0.05783149]] having: 100\n",
      "--------------------------\n",
      "fold 3\n",
      "[[-0.86939937  1.40880986  1.38417949 ...  1.27250082  1.29450258\n",
      "   0.05783149]\n",
      " [-0.88603225  1.51464699  1.45229855 ...  1.44839558  1.39220246\n",
      "   0.05783149]\n",
      " [-0.91483258  1.62048413  1.55447716 ...  1.56565875  1.47429533\n",
      "   0.05783149]\n",
      " ...\n",
      " [ 0.11512288  0.61503135  0.63486972 ...  0.9793429   0.51071857\n",
      "   0.05783149]\n",
      " [ 0.22332012  0.61503135  0.70298879 ...  0.92071132  0.52632558\n",
      "   0.05783149]\n",
      " [ 0.22233988  0.59739183  0.83922693 ...  0.86207973  0.50322721\n",
      "   0.05783149]] having: 200\n",
      "[[ -0.79140015   0.66794992   0.66892926 ...   0.15850072   0.62152834\n",
      "    0.05783149]\n",
      " [ -0.71698123   0.66794992   0.60081019 ...   0.27576389   0.62184048\n",
      "    0.05783149]\n",
      " [ -0.6571167    0.66794992   0.63486972 ...   0.33439547   0.63900819\n",
      "    0.05783149]\n",
      " ...\n",
      " [  0.12883512  -0.30222382  -0.31879724 ...  -0.31055196  -0.21500736\n",
      "    0.05783149]\n",
      " [  0.96665167   0.68558944   0.70298879 ...   0.62755339   0.29284472\n",
      "    0.05783149]\n",
      " [ -0.0473281    0.79142657   0.77110786 ...   0.68618498  -8.36186624\n",
      "  -17.29161647]] having: 100\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for train_index,test_index in kf.split(x_reshaped,y1):\n",
    "    print(\"fold\",i)\n",
    "    x_train,x_test=x[train_index],x[test_index]\n",
    "    y_train,y_test=y1[train_index],y1[test_index]\n",
    "    print(x_train,\"having:\",len(x_train))\n",
    "    print(x_test,\"having:\",len(x_test))\n",
    "    print(\"--------------------------\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y1))\n",
    "print(np.unique(y1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y2))\n",
    "print(np.unique(y2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input,Add,Dense,Activation,ZeroPadding1D,Flatten,Conv1D,AveragePooling1D,MaxPooling1D\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X,f,filters):\n",
    "    F1,F2,F3 = filters\n",
    "    X_shortcut = X\n",
    "    \n",
    "    #first layer\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=1, padding='same')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    #second layer\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=1, padding='same')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    #third layer\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=1, padding='same')(X)\n",
    "    \n",
    "    #final step: add shortcut value(X_shortcut)\n",
    "    X = Add()([X,X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X,f,filters,s=2):\n",
    "    F1,F2,F3 = filters\n",
    "    X_shortcut = X\n",
    "    \n",
    "    #first layer\n",
    "    X = Conv2D(filters=F1, kernel_size=(1,1), strides=(s, s), padding='same')(X)\n",
    "    X = BatchNormalization(axis=3)(X) #normalisation on channels\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    #second layer\n",
    "    X = Conv2D(filters=F2, kernel_size=(f,f), strides=(1,1), padding='same')(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    #third layer\n",
    "    X = Conv2D(filters=F3, kernel_size=(1,1), strides=(1,1), padding='same')(X)\n",
    "    \n",
    "    #adding convolutional layer in shortcut path#\n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=1, strides=(s, s), padding='same')( X_shortcut)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    #final step: add shortcut value(X_shortcut)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet(input_shape=(150, 150, 3), classes=6):\n",
    "    \n",
    "    #define the input_shape\n",
    "    X_input=Input(input_shape)\n",
    "    \n",
    "    #zero-padding\n",
    "    X=ZeroPadding2D(3, 3)(X_input)\n",
    "    \n",
    "    #Step 1\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling1D((3, 3), strides=(2, 2))(X)\n",
    "    \n",
    "    #Step 2\n",
    "    X = convolutional_block(X, f=3, filters=[64,64,256], s=1)\n",
    "    X = identity_block(X,3,[64,64,256])\n",
    "    X = identity_block(X,3,[64,64,256])\n",
    "    \n",
    "    #step 3\n",
    "    X = convolutional_block(X, f=3, filters=[128,128,512], s=2)\n",
    "    X = identity_block(X, 3, [128,128,512])\n",
    "    X = identity_block(X, 3, [128,128,512])\n",
    "    X = identity_block(X, 3, [128,128,512])\n",
    "    \n",
    "    #step 4\n",
    "    X = convolutional_block(X, f=3, filters=[256,256,1024], s=2)\n",
    "    X = identity_block(X, 3, [256,256,1024])\n",
    "    X = identity_block(X, 3, [256,256,1024])\n",
    "    X = identity_block(X, 3, [256,256,1024])\n",
    "    X = identity_block(X, 3, [256,256,1024])\n",
    "    \n",
    "    #step 5\n",
    "    X = convolutional_block(X, f=3, filters=[512,512,2048], s=2)\n",
    "    X = identity_block(X, 3, [512,512,2048])\n",
    "    X = identity_block(X, 3, [512,512,2048])\n",
    "    \n",
    "    #AVGPOOL\n",
    "    X = AveragePooling2D((1, 1), name=\"avg_pool\")(X)\n",
    "    \n",
    "    #Output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc'+str(classes), kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    model=Model(inputs=X_input, outputs=X, name='ResNet')\n",
    "    return model\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ResNet(input_shape=(150, 150,1),classes=6)\n",
    "model.compile(optimizer='adam',\n",
    "               loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "               loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 12, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_1 (ZeroPadding1D (None, 18, 1)        0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 6, 64)        512         zero_padding1d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 2, 64)        0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 2, 64)        4160        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 2, 64)        0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 2, 64)        12352       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 2, 64)        0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 2, 256)       16640       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 2, 256)       16640       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 2, 256)       0           conv1d_53[0][0]                  \n",
      "                                                                 conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 2, 256)       0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 2, 64)        16448       activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 2, 64)        0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 2, 64)        12352       activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 2, 64)        0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 2, 256)       16640       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 2, 256)       0           conv1d_57[0][0]                  \n",
      "                                                                 activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 2, 256)       0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 2, 64)        16448       activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 2, 64)        0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 2, 64)        12352       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 2, 64)        0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 2, 256)       16640       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 2, 256)       0           conv1d_60[0][0]                  \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 2, 256)       0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 1, 128)       32896       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 1, 128)       0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 1, 128)       49280       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 1, 128)       0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 1, 512)       66048       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 1, 512)       131584      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 1, 512)       0           conv1d_63[0][0]                  \n",
      "                                                                 conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 1, 512)       0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 1, 128)       65664       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 1, 128)       0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, 1, 128)       49280       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 1, 128)       0           conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, 1, 512)       66048       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 1, 512)       0           conv1d_67[0][0]                  \n",
      "                                                                 activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 1, 512)       0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, 1, 128)       65664       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1, 128)       0           conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, 1, 128)       49280       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 1, 128)       0           conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_70 (Conv1D)              (None, 1, 512)       66048       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 1, 512)       0           conv1d_70[0][0]                  \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 1, 512)       0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_71 (Conv1D)              (None, 1, 128)       65664       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 1, 128)       0           conv1d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_72 (Conv1D)              (None, 1, 128)       49280       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 1, 128)       0           conv1d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_73 (Conv1D)              (None, 1, 512)       66048       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 1, 512)       0           conv1d_73[0][0]                  \n",
      "                                                                 activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 1, 512)       0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_74 (Conv1D)              (None, 1, 256)       131328      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 1, 256)       0           conv1d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_75 (Conv1D)              (None, 1, 256)       196864      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 1, 256)       0           conv1d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_76 (Conv1D)              (None, 1, 1024)      263168      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_77 (Conv1D)              (None, 1, 1024)      525312      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 1, 1024)      0           conv1d_76[0][0]                  \n",
      "                                                                 conv1d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 1, 1024)      0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_78 (Conv1D)              (None, 1, 256)       262400      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 1, 256)       0           conv1d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_79 (Conv1D)              (None, 1, 256)       196864      activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 1, 256)       0           conv1d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_80 (Conv1D)              (None, 1, 1024)      263168      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 1, 1024)      0           conv1d_80[0][0]                  \n",
      "                                                                 activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 1, 1024)      0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_81 (Conv1D)              (None, 1, 256)       262400      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 1, 256)       0           conv1d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_82 (Conv1D)              (None, 1, 256)       196864      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 1, 256)       0           conv1d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_83 (Conv1D)              (None, 1, 1024)      263168      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 1, 1024)      0           conv1d_83[0][0]                  \n",
      "                                                                 activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 1, 1024)      0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_84 (Conv1D)              (None, 1, 256)       262400      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 1, 256)       0           conv1d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_85 (Conv1D)              (None, 1, 256)       196864      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 1, 256)       0           conv1d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_86 (Conv1D)              (None, 1, 1024)      263168      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 1, 1024)      0           conv1d_86[0][0]                  \n",
      "                                                                 activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 1, 1024)      0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_87 (Conv1D)              (None, 1, 256)       262400      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 1, 256)       0           conv1d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_88 (Conv1D)              (None, 1, 256)       196864      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 1, 256)       0           conv1d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_89 (Conv1D)              (None, 1, 1024)      263168      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 1, 1024)      0           conv1d_89[0][0]                  \n",
      "                                                                 activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 1, 1024)      0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_90 (Conv1D)              (None, 1, 512)       524800      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 1, 512)       0           conv1d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_91 (Conv1D)              (None, 1, 512)       786944      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 1, 512)       0           conv1d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_92 (Conv1D)              (None, 1, 2048)      1050624     activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_93 (Conv1D)              (None, 1, 2048)      2099200     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 1, 2048)      0           conv1d_92[0][0]                  \n",
      "                                                                 conv1d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 1, 2048)      0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_94 (Conv1D)              (None, 1, 512)       1049088     activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 1, 512)       0           conv1d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_95 (Conv1D)              (None, 1, 512)       786944      activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 1, 512)       0           conv1d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_96 (Conv1D)              (None, 1, 2048)      1050624     activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 1, 2048)      0           conv1d_96[0][0]                  \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 1, 2048)      0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_97 (Conv1D)              (None, 1, 512)       1049088     activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 1, 512)       0           conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_98 (Conv1D)              (None, 1, 512)       786944      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 1, 512)       0           conv1d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_99 (Conv1D)              (None, 1, 2048)      1050624     activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 1, 2048)      0           conv1d_99[0][0]                  \n",
      "                                                                 activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 1, 2048)      0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling1D)     (None, 1, 2048)      0           activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc6 (Dense)                     (None, 6)            12294       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,217,542\n",
      "Trainable params: 15,217,542\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 16s 592ms/step - loss: 1.6126 - accuracy: 0.2996 - val_loss: 7.0693 - val_accuracy: 0.2424\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 7s 488ms/step - loss: 1.1425 - accuracy: 0.5894 - val_loss: 4.0050 - val_accuracy: 0.2424\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 7s 499ms/step - loss: 0.5973 - accuracy: 0.5963 - val_loss: 14.7185 - val_accuracy: 0.2424\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 7s 512ms/step - loss: 0.6075 - accuracy: 0.6583 - val_loss: 15.5093 - val_accuracy: 0.2424\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 8s 567ms/step - loss: 0.4977 - accuracy: 0.6792 - val_loss: 15.8714 - val_accuracy: 0.2424\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 7s 519ms/step - loss: 0.4939 - accuracy: 0.7191 - val_loss: 17.9703 - val_accuracy: 0.2424\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 7s 530ms/step - loss: 0.5458 - accuracy: 0.5502 - val_loss: 32.5154 - val_accuracy: 0.2424\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 7s 519ms/step - loss: 0.5352 - accuracy: 0.6401 - val_loss: 5.9360 - val_accuracy: 0.2273\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 7s 505ms/step - loss: 0.5702 - accuracy: 0.7123 - val_loss: 15.6712 - val_accuracy: 0.2424\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 7s 493ms/step - loss: 0.4713 - accuracy: 0.7499 - val_loss: 19.2666 - val_accuracy: 0.2424\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 8s 542ms/step - loss: 0.4570 - accuracy: 0.7147 - val_loss: 11.1183 - val_accuracy: 0.2424\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 8s 548ms/step - loss: 0.4677 - accuracy: 0.7004 - val_loss: 19.4488 - val_accuracy: 0.2424\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 7s 521ms/step - loss: 0.5031 - accuracy: 0.7059 - val_loss: 24.5769 - val_accuracy: 0.2424\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 7s 499ms/step - loss: 0.4437 - accuracy: 0.7219 - val_loss: 26.2662 - val_accuracy: 0.2424\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 7s 518ms/step - loss: 0.4683 - accuracy: 0.7253 - val_loss: 27.7480 - val_accuracy: 0.2424\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 7s 519ms/step - loss: 0.4014 - accuracy: 0.7507 - val_loss: 28.9600 - val_accuracy: 0.2424\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 7s 526ms/step - loss: 0.4041 - accuracy: 0.7620 - val_loss: 28.6826 - val_accuracy: 0.2424\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 8s 536ms/step - loss: 0.4129 - accuracy: 0.7526 - val_loss: 30.5228 - val_accuracy: 0.2424\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 7s 507ms/step - loss: 0.4330 - accuracy: 0.7528 - val_loss: 15.2641 - val_accuracy: 0.2424\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 8s 556ms/step - loss: 0.4220 - accuracy: 0.7368 - val_loss: 21.0959 - val_accuracy: 0.2424\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 8s 576ms/step - loss: 0.4037 - accuracy: 0.7805 - val_loss: 29.0712 - val_accuracy: 0.2424\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 7s 540ms/step - loss: 0.3976 - accuracy: 0.7645 - val_loss: 31.5802 - val_accuracy: 0.2424\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 8s 538ms/step - loss: 0.4308 - accuracy: 0.7568 - val_loss: 37.2953 - val_accuracy: 0.2424\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 7s 519ms/step - loss: 0.4193 - accuracy: 0.6966 - val_loss: 39.7514 - val_accuracy: 0.2424\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 7s 508ms/step - loss: 0.4617 - accuracy: 0.7171 - val_loss: 44.3896 - val_accuracy: 0.2424\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 7s 509ms/step - loss: 0.4343 - accuracy: 0.7410 - val_loss: 56.7069 - val_accuracy: 0.2424\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 7s 496ms/step - loss: 0.5137 - accuracy: 0.6592 - val_loss: 28.9312 - val_accuracy: 0.2424\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 7s 503ms/step - loss: 0.8788 - accuracy: 0.7382 - val_loss: 16.7331 - val_accuracy: 0.2424\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 7s 511ms/step - loss: 0.8314 - accuracy: 0.6086 - val_loss: 21.5295 - val_accuracy: 0.2424\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 7s 505ms/step - loss: 0.5489 - accuracy: 0.6387 - val_loss: 11.7475 - val_accuracy: 0.2424\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 7s 510ms/step - loss: 0.5374 - accuracy: 0.5641 - val_loss: 17.1235 - val_accuracy: 0.2424\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 7s 500ms/step - loss: 0.5365 - accuracy: 0.6513 - val_loss: 12.3685 - val_accuracy: 0.2424\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 7s 511ms/step - loss: 0.5697 - accuracy: 0.7021 - val_loss: 147.8060 - val_accuracy: 0.2424\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 7s 500ms/step - loss: 0.6017 - accuracy: 0.6466 - val_loss: 13.3793 - val_accuracy: 0.2424\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 7s 514ms/step - loss: 0.5929 - accuracy: 0.5667 - val_loss: 15.5593 - val_accuracy: 0.2424\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 7s 500ms/step - loss: 0.4980 - accuracy: 0.6038 - val_loss: 27.1477 - val_accuracy: 0.2424\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 7s 514ms/step - loss: 0.5348 - accuracy: 0.6008 - val_loss: 36.8103 - val_accuracy: 0.2424\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 7s 502ms/step - loss: 0.5016 - accuracy: 0.7218 - val_loss: 38.9987 - val_accuracy: 0.2424\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 7s 481ms/step - loss: 0.4713 - accuracy: 0.7165 - val_loss: 39.1629 - val_accuracy: 0.2424\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 7s 498ms/step - loss: 0.4194 - accuracy: 0.6538 - val_loss: 40.1364 - val_accuracy: 0.2424\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 7s 510ms/step - loss: 0.4602 - accuracy: 0.7503 - val_loss: 57.6245 - val_accuracy: 0.2424\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 7s 498ms/step - loss: 0.4375 - accuracy: 0.6960 - val_loss: 60.7244 - val_accuracy: 0.2424\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 7s 503ms/step - loss: 0.4746 - accuracy: 0.6965 - val_loss: 67.3634 - val_accuracy: 0.2424\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 7s 514ms/step - loss: 0.4226 - accuracy: 0.7696 - val_loss: 89.7042 - val_accuracy: 0.2424\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 7s 501ms/step - loss: 0.3543 - accuracy: 0.8172 - val_loss: 85.3632 - val_accuracy: 0.2424\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 7s 479ms/step - loss: 0.4361 - accuracy: 0.7617 - val_loss: 76.3825 - val_accuracy: 0.2424\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 7s 516ms/step - loss: 0.4229 - accuracy: 0.7534 - val_loss: 75.7558 - val_accuracy: 0.2424\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 7s 505ms/step - loss: 0.3788 - accuracy: 0.7572 - val_loss: 65.2364 - val_accuracy: 0.2424\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 7s 502ms/step - loss: 0.5233 - accuracy: 0.7266 - val_loss: 23.8626 - val_accuracy: 0.2424\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 7s 501ms/step - loss: 0.4868 - accuracy: 0.7314 - val_loss: 19.6144 - val_accuracy: 0.2424\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 7s 528ms/step - loss: 0.5127 - accuracy: 0.6296 - val_loss: 38.2255 - val_accuracy: 0.2424\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 7s 527ms/step - loss: 0.5055 - accuracy: 0.6168 - val_loss: 46.0839 - val_accuracy: 0.2424\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 7s 504ms/step - loss: 0.5045 - accuracy: 0.7061 - val_loss: 48.0719 - val_accuracy: 0.2424\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 7s 489ms/step - loss: 0.5394 - accuracy: 0.6110 - val_loss: 48.7203 - val_accuracy: 0.2424\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 7s 492ms/step - loss: 0.5076 - accuracy: 0.7081 - val_loss: 49.9551 - val_accuracy: 0.2424\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 7s 497ms/step - loss: 0.5090 - accuracy: 0.6415 - val_loss: 52.5895 - val_accuracy: 0.2424\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 7s 488ms/step - loss: 0.4778 - accuracy: 0.6224 - val_loss: 58.4216 - val_accuracy: 0.2424\n",
      "Epoch 58/100\n",
      "14/14 [==============================] - 7s 500ms/step - loss: 0.4502 - accuracy: 0.7128 - val_loss: 66.0739 - val_accuracy: 0.2424\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 7s 481ms/step - loss: 0.4264 - accuracy: 0.7353 - val_loss: 78.9201 - val_accuracy: 0.2424\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 7s 511ms/step - loss: 0.6698 - accuracy: 0.6853 - val_loss: 244.7491 - val_accuracy: 0.2424\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 7s 525ms/step - loss: 0.7613 - accuracy: 0.5696 - val_loss: 7.9130 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "14/14 [==============================] - 7s 488ms/step - loss: 0.6147 - accuracy: 0.5580 - val_loss: 41.9320 - val_accuracy: 0.2424\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 7s 512ms/step - loss: 0.4666 - accuracy: 0.6288 - val_loss: 59.3665 - val_accuracy: 0.2424\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 7s 479ms/step - loss: 0.5033 - accuracy: 0.6949 - val_loss: 62.1467 - val_accuracy: 0.2424\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 7s 504ms/step - loss: 0.4824 - accuracy: 0.6618 - val_loss: 64.1868 - val_accuracy: 0.2424\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 7s 487ms/step - loss: 0.4915 - accuracy: 0.7380 - val_loss: 57.6600 - val_accuracy: 0.2424\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 7s 503ms/step - loss: 0.4464 - accuracy: 0.7524 - val_loss: 56.3921 - val_accuracy: 0.2424\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 7s 492ms/step - loss: 0.4463 - accuracy: 0.7156 - val_loss: 55.7329 - val_accuracy: 0.2424\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 7s 487ms/step - loss: 0.4020 - accuracy: 0.7507 - val_loss: 54.5545 - val_accuracy: 0.2424\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 7s 479ms/step - loss: 0.4185 - accuracy: 0.7298 - val_loss: 54.2749 - val_accuracy: 0.2424\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 7s 499ms/step - loss: 0.4592 - accuracy: 0.6892 - val_loss: 53.9879 - val_accuracy: 0.2424\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 7s 489ms/step - loss: 0.4562 - accuracy: 0.7485 - val_loss: 51.0918 - val_accuracy: 0.2424\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 7s 480ms/step - loss: 0.4371 - accuracy: 0.7237 - val_loss: 50.7430 - val_accuracy: 0.2424\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 7s 471ms/step - loss: 0.4842 - accuracy: 0.6921 - val_loss: 56.4394 - val_accuracy: 0.2424\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 7s 496ms/step - loss: 0.3818 - accuracy: 0.7978 - val_loss: 57.1100 - val_accuracy: 0.2424\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 7s 487ms/step - loss: 0.4211 - accuracy: 0.7822 - val_loss: 59.7092 - val_accuracy: 0.2424\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 7s 479ms/step - loss: 0.4766 - accuracy: 0.6838 - val_loss: 56.5935 - val_accuracy: 0.2424\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 7s 465ms/step - loss: 0.5690 - accuracy: 0.6663 - val_loss: 85.7711 - val_accuracy: 0.2424\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 7s 493ms/step - loss: 0.5396 - accuracy: 0.5983 - val_loss: 8.9027 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 7s 486ms/step - loss: 0.8946 - accuracy: 0.5821 - val_loss: 43.8068 - val_accuracy: 0.2424\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 7s 502ms/step - loss: 0.5038 - accuracy: 0.5651 - val_loss: 61.9665 - val_accuracy: 0.2424\n",
      "Epoch 82/100\n",
      "14/14 [==============================] - 7s 500ms/step - loss: 0.4975 - accuracy: 0.6146 - val_loss: 47.4136 - val_accuracy: 0.2424\n",
      "Epoch 83/100\n",
      "14/14 [==============================] - 7s 471ms/step - loss: 0.4320 - accuracy: 0.7634 - val_loss: 50.0026 - val_accuracy: 0.2424\n",
      "Epoch 84/100\n",
      "14/14 [==============================] - 7s 483ms/step - loss: 0.5387 - accuracy: 0.6719 - val_loss: 48.0426 - val_accuracy: 0.2424\n",
      "Epoch 85/100\n",
      "14/14 [==============================] - 7s 486ms/step - loss: 0.5743 - accuracy: 0.6851 - val_loss: 35.8682 - val_accuracy: 0.2424\n",
      "Epoch 86/100\n",
      "14/14 [==============================] - 7s 481ms/step - loss: 0.4996 - accuracy: 0.6846 - val_loss: 95.7372 - val_accuracy: 0.2424\n",
      "Epoch 87/100\n",
      "14/14 [==============================] - 7s 475ms/step - loss: 0.4643 - accuracy: 0.6825 - val_loss: 104.8052 - val_accuracy: 0.2424\n",
      "Epoch 88/100\n",
      "14/14 [==============================] - 7s 484ms/step - loss: 0.4419 - accuracy: 0.6974 - val_loss: 108.4550 - val_accuracy: 0.2424\n",
      "Epoch 89/100\n",
      "14/14 [==============================] - 7s 474ms/step - loss: 0.4158 - accuracy: 0.7618 - val_loss: 111.0166 - val_accuracy: 0.2424\n",
      "Epoch 90/100\n",
      "14/14 [==============================] - 7s 519ms/step - loss: 0.4456 - accuracy: 0.7207 - val_loss: 52.8246 - val_accuracy: 0.2424\n",
      "Epoch 91/100\n",
      "14/14 [==============================] - 7s 489ms/step - loss: 0.4390 - accuracy: 0.7413 - val_loss: 21.7812 - val_accuracy: 0.2424\n",
      "Epoch 92/100\n",
      "14/14 [==============================] - 7s 477ms/step - loss: 0.4174 - accuracy: 0.7586 - val_loss: 19.9837 - val_accuracy: 0.2424\n",
      "Epoch 93/100\n",
      "14/14 [==============================] - 7s 492ms/step - loss: 0.4759 - accuracy: 0.7145 - val_loss: 21.4483 - val_accuracy: 0.2424\n",
      "Epoch 94/100\n",
      "14/14 [==============================] - 7s 482ms/step - loss: 0.4518 - accuracy: 0.7234 - val_loss: 22.2332 - val_accuracy: 0.2424\n",
      "Epoch 95/100\n",
      "14/14 [==============================] - 7s 483ms/step - loss: 0.4123 - accuracy: 0.7447 - val_loss: 22.5142 - val_accuracy: 0.2424\n",
      "Epoch 96/100\n",
      "14/14 [==============================] - 7s 472ms/step - loss: 0.4095 - accuracy: 0.7409 - val_loss: 22.5746 - val_accuracy: 0.2424\n",
      "Epoch 97/100\n",
      "14/14 [==============================] - 7s 474ms/step - loss: 0.4237 - accuracy: 0.7388 - val_loss: 22.7022 - val_accuracy: 0.2424\n",
      "Epoch 98/100\n",
      "14/14 [==============================] - 7s 487ms/step - loss: 0.3910 - accuracy: 0.7517 - val_loss: 22.7151 - val_accuracy: 0.2424\n",
      "Epoch 99/100\n",
      "14/14 [==============================] - 7s 471ms/step - loss: 0.4273 - accuracy: 0.7353 - val_loss: 22.8676 - val_accuracy: 0.2424\n",
      "Epoch 100/100\n",
      "14/14 [==============================] - 7s 504ms/step - loss: 0.4099 - accuracy: 0.7476 - val_loss: 23.0742 - val_accuracy: 0.2424\n"
     ]
    }
   ],
   "source": [
    "model_history=model.fit(x_train, y_train,validation_split=.33,batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.pred=model.predict(x_test)\n",
    "y_pred=np.pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      50.0\n",
      "           2       0.00      0.00      0.00       0.0\n",
      "           3       0.00      0.00      0.00       0.0\n",
      "           4       0.00      0.00      0.00       0.0\n",
      "           5       0.00      0.00      0.00      50.0\n",
      "\n",
      "    accuracy                           0.00     100.0\n",
      "   macro avg       0.00      0.00      0.00     100.0\n",
      "weighted avg       0.00      0.00      0.00     100.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abulf\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\abulf\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(69.0, 0.5, 'Truth')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "cm=tf.math.confusion_matrix(labels=y_test,predictions=y_pred)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.heatmap(cm,annot=True,fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc=(12+13+11+14+11+19)/(14+14+13+15+14+20)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
